{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11p45xZKM7m_6_fOwbjg6OfIzGMbukTbM","timestamp":1679630195322},{"file_id":"1MRMIevmhSyRvuZCBSjzXR5t7A8oBc_XJ","timestamp":1679160739248},{"file_id":"14FB8KgLccw0YCmmP4UJKtQq6svMhYJbK","timestamp":1679159606890},{"file_id":"18IceUTj0whQgabciDd9cB7mlBe4Oskf1","timestamp":1679158620563},{"file_id":"1Y3xKjDo2S5GiTczuvhhGSvyr4bZT-l2M","timestamp":1679124607327},{"file_id":"178_0geD9fcvOKpKkBffmj2TnOokFp6Vk","timestamp":1679120765034},{"file_id":"1zREZNWFEwPYx08zuWLAN6BTSk-y6aoIc","timestamp":1679117679549},{"file_id":"1LAF4gnn7vjXy7B73ZolZOH6E5aqNIddK","timestamp":1679095991144},{"file_id":"12-WAIVBAdZtQTJvXglIZrhVHYFziBwjC","timestamp":1678421752975}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JhD1FafIaZiH","executionInfo":{"status":"ok","timestamp":1679666254961,"user_tz":240,"elapsed":1524,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"0f249f26-2dbd-4a79-f7f4-0484f205b807"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CLIP_Steering'...\n","remote: Enumerating objects: 25, done.\u001b[K\n","remote: Counting objects: 100% (25/25), done.\u001b[K\n","remote: Compressing objects: 100% (19/19), done.\u001b[K\n","remote: Total 25 (delta 8), reused 20 (delta 3), pack-reused 0\u001b[K\n","Unpacking objects: 100% (25/25), 208.02 KiB | 1.03 MiB/s, done.\n","/content/CLIP_Steering\n"]}],"source":["!git clone https://github.com/fengqingthu/CLIP_Steering.git\n","%cd /content/CLIP_Steering/"]},{"cell_type":"markdown","source":["Install clip + stylegan2 dependencies"],"metadata":{"id":"kV0COHSPb_FH"}},{"cell_type":"code","source":["!conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"id":"9YWp1gBBi8tM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679666280303,"user_tz":240,"elapsed":23315,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"d94650d8-9f85-42b5-aee4-ed4f1e5fc3a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: conda: command not found\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy) (0.2.6)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-4vat19z2\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-4vat19z2\n","  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (6.1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (4.65.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (1.13.1+cu116)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (0.14.1+cu116)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->clip==1.0) (0.2.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (4.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (1.22.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (8.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (2.27.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369398 sha256=6df38097f62dc5db6e9b48679a2536d0c68f661a61619b39e5231b618c134832\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-7_hqdntb/wheels/c8/e4/e1/11374c111387672fc2068dfbe0d4b424cb9cdd1b2e184a71b5\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]}]},{"cell_type":"code","source":["!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"],"metadata":{"id":"FZBnAjACbhKm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679666295625,"user_tz":240,"elapsed":11330,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"36d6a045-89f5-4301-81e8-81e4f51a599e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (8.1.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n","Collecting pyspng\n","  Downloading pyspng-0.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ninja\n","  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting imageio-ffmpeg==0.4.3\n","  Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests) (1.26.15)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pyspng) (1.22.4)\n","Installing collected packages: ninja, pyspng, imageio-ffmpeg\n","  Attempting uninstall: imageio-ffmpeg\n","    Found existing installation: imageio-ffmpeg 0.4.8\n","    Uninstalling imageio-ffmpeg-0.4.8:\n","      Successfully uninstalled imageio-ffmpeg-0.4.8\n","Successfully installed imageio-ffmpeg-0.4.3 ninja-1.11.1 pyspng-0.1.1\n"]}]},{"cell_type":"markdown","source":["Clone the CLIP repo and test that it works"],"metadata":{"id":"st3m7E1AlIj-"}},{"cell_type":"code","source":["!git clone https://github.com/openai/CLIP.git\n","%cd CLIP"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wGT4D-FVkICz","executionInfo":{"status":"ok","timestamp":1679666299789,"user_tz":240,"elapsed":1190,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"af05d5f6-9b68-4f74-c489-b2ccf598995f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CLIP'...\n","remote: Enumerating objects: 243, done.\u001b[K\n","remote: Total 243 (delta 0), reused 0 (delta 0), pack-reused 243\u001b[K\n","Receiving objects: 100% (243/243), 8.92 MiB | 22.07 MiB/s, done.\n","Resolving deltas: 100% (123/123), done.\n","/content/CLIP_Steering/CLIP\n"]}]},{"cell_type":"code","source":["import torch\n","import clip\n","from PIL import Image\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n","text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n","\n","with torch.no_grad():\n","    image_features = model.encode_image(image)\n","    text_features = model.encode_text(text)\n","    \n","    logits_per_image, logits_per_text = model(image, text)\n","    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n","\n","print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_qHzPE4BjJpW","executionInfo":{"status":"ok","timestamp":1679666330489,"user_tz":240,"elapsed":28532,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"c08cc09d-403a-400a-fa9e-c033ce4f53bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 94.2MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Label probs: [[0.9927   0.004253 0.002968]]\n"]}]},{"cell_type":"code","source":["%cd /content/CLIP_Steering/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mheX4PDHkSbS","executionInfo":{"status":"ok","timestamp":1679666373706,"user_tz":240,"elapsed":587,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"14c22c76-ba57-48b6-f9ce-bb12d23bb239"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/CLIP_Steering\n"]}]},{"cell_type":"markdown","source":["Clone the styleGAN2 repo and test that it works"],"metadata":{"id":"8uvy7C3PnoLj"}},{"cell_type":"code","source":["!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n","%cd /content/CLIP_Steering/stylegan2-ada-pytorch/"],"metadata":{"id":"Vi3jpP30nr9J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679666375940,"user_tz":240,"elapsed":723,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"f23d9565-7f97-4e8c-fa78-de24ae602bc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'stylegan2-ada-pytorch'...\n","remote: Enumerating objects: 128, done.\u001b[K\n","remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n","Receiving objects: 100% (128/128), 1.12 MiB | 20.87 MiB/s, done.\n","Resolving deltas: 100% (57/57), done.\n","/content/CLIP_Steering/stylegan2-ada-pytorch\n"]}]},{"cell_type":"markdown","source":["Download pretrained styleGAN2 model. A couple of more options on: https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/"],"metadata":{"id":"pvc5d1TylD9B"}},{"cell_type":"code","source":["!mkdir /content/CLIP_Steering/pretrained & wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqcat.pkl -O /content/CLIP_Steering/pretrained/afhqcat.pkl"],"metadata":{"id":"KeJjyATqcDgo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679666387276,"user_tz":240,"elapsed":9311,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"c8a671f2-aff6-4ec2-a28a-81f1321c5ac8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-03-24 13:59:37--  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqcat.pkl\n","Resolving nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)... 52.222.139.28, 52.222.139.123, 52.222.139.85, ...\n","Connecting to nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)|52.222.139.28|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 363959591 (347M) [binary/octet-stream]\n","Saving to: ‘/content/CLIP_Steering/pretrained/afhqcat.pkl’\n","\n","/content/CLIP_Steer 100%[===================>] 347.10M  41.0MB/s    in 8.5s    \n","\n","2023-03-24 13:59:46 (40.8 MB/s) - ‘/content/CLIP_Steering/pretrained/afhqcat.pkl’ saved [363959591/363959591]\n","\n"]}]},{"cell_type":"code","source":["#Generate face images from pretrained stylegan on faces.\n","#change  seed numbers to get different out images if you like. Try Metfaces for diff variety of face images\n","#check out folder to verify that generated images have appeared\n","!python generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\n","    --network=../pretrained/afhqcat.pkl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ZG9PNXeki8t","executionInfo":{"status":"ok","timestamp":1679666476066,"user_tz":240,"elapsed":75743,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"54a6e6ca-c8b0-438a-cebf-7d2155ec3bc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading networks from \"../pretrained/afhqcat.pkl\"...\n","Generating image for seed 85 (0/4) ...\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","Generating image for seed 265 (1/4) ...\n","Generating image for seed 297 (2/4) ...\n","Generating image for seed 849 (3/4) ...\n"]}]},{"cell_type":"code","source":["%cd /content/CLIP_Steering/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sf4x5BWnpsEq","executionInfo":{"status":"ok","timestamp":1679666485393,"user_tz":240,"elapsed":544,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"effd1f1d-e457-4086-c37f-c4047b056a02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/CLIP_Steering\n"]}]},{"cell_type":"code","source":["!pwd # Make sure the current working directory is /content/CLIP_Steering"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PU5MPBazqNKq","executionInfo":{"status":"ok","timestamp":1679666487607,"user_tz":240,"elapsed":519,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"813a7226-3646-424f-f62e-beb763b08a98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/CLIP_Steering\n"]}]},{"cell_type":"markdown","source":["An easy fix to make the dnnlib and torch_utils modules within the styleGAN2 repo accessible from PYTHONPATH - simply copy them to the project root"],"metadata":{"id":"RRWE5T3ev2sl"}},{"cell_type":"code","source":["!cp -a stylegan2-ada-pytorch/dnnlib dnnlib\n","!cp -a stylegan2-ada-pytorch/torch_utils torch_utils"],"metadata":{"id":"2OrOhPgLtqri"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following code is from the file ganalyze_with_clip.py"],"metadata":{"id":"CcUGd3kPwDje"}},{"cell_type":"code","source":["import os\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'MAX_SPLIT_SIZE_1=32MB'"],"metadata":{"id":"-pQ2AUXxDFyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","torch.cuda.empty_cache()"],"metadata":{"id":"0759stIpDH9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nv-WC_ujxd1r","executionInfo":{"status":"ok","timestamp":1679666516315,"user_tz":240,"elapsed":20008,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"110a39f8-e4a7-49e1-85e3-1d3cb0047667"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Here is how to change the GAN model\n","gan_model_path = 'pretrained/afhqcat.pkl'\n","\n","# Here is how we specify the desired attributes\n","attributes = [\"a cute cat\",\"an ugly cat\"]\n","#attributes = [\"an old face\", \"a young face\", \"a happy face\",\"a sad face\"]\n","class_index = 0 # which attribute do we want to maximize, ie. attribute number 0, or attribute number 1 etc.\n","\n","# Here is where to store the checkpoints i.e. weights \n","# checkpoint_dir = f'checkpoints/results_maximize_{attributes[class_index]}_probability'\n","checkpoint_dir = f'/content/drive/MyDrive/evilcat/results_maximize_{attributes[class_index]}_probability'\n"],"metadata":{"id":"P3Pm6qcOespK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import argparse\n","import json\n","import subprocess\n","import sys\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torchvision.transforms as torch_transforms\n","import pickle\n","import os\n","import pathlib\n","import clip\n","import matplotlib.pyplot as plt\n","\n","sys.path.append(os.path.abspath(os.getcwd()))\n","sys.path.append('/content/CLIP_Steering/stylegan2-ada-pytorch/')\n","\n","import torch_utils\n","import ganalyze_transformations as transformations\n","import ganalyze_common_utils as common\n","from clip_classifier_utils import SimpleTokenizer\n","\n","import logging\n","\n","logging.basicConfig(\n","    format='%(asctime)s %(levelname)-8s %(message)s',\n","    level=logging.INFO,\n","    datefmt='%Y-%m-%d %H:%M:%S',\n",")\n","\n","logger = logging.getLogger(__name__)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def plot_save_image(img, out_dir):\n","    img = (gan_images.permute(0, 2, 3, 1)).clamp(0, 255).to(torch.uint8)\n","    img_np = img.detach().cpu().numpy().squeeze()\n","\n","    fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(10, 6))\n","    for i in range(6): # By defualt batchsize= 6\n","        row = i // 3\n","        col = i % 3\n","        ax[row, col].imshow(img_np[i])\n","    plt.show()\n","    \n","    for i in range(6):\n","      filename = f\"image_{batch_start}_{i}.png\"\n","      plt.imsave(f\"{checkpoint_dir}/{filename}\", img_np[i])\n","\n","def gan_output_transform(imgs):\n","    # Input:\n","    # img: NCHW\n","    #\n","    # Output\n","    # img_np: HWC RGB image\n","\n","    imgs = (imgs * 127.5 + 128).clamp(0, 255).float()\n","    return imgs\n","\n","\n","def clip_input_transform(images):\n","    # Input\n","    # img_np: torch tensor of shape NHWC, RGB image\n","    #\n","    # Output\n","    # image_input: torch tensor of shape NHWC\n","\n","    image_mean = (0.48145466, 0.4578275, 0.40821073)\n","    image_std = (0.26862954, 0.26130258, 0.27577711)\n","\n","    transform = torch.nn.Sequential(\n","        torch_transforms.Resize((256, 256)),\n","        torch_transforms.CenterCrop((224, 224)),\n","        torch_transforms.Normalize(image_mean, image_std),\n","    )\n","\n","    return transform(images)\n","\n","def get_clip_scores(image_inputs, encoded_text, model, class_index=0):\n","    #TODO: clarify class index\n","    image_inputs = clip_input_transform(image_inputs).to(device)\n","    image_feats = model.encode_image(image_inputs).float()\n","    image_feats = F.normalize(image_feats, p=2, dim=-1)\n","\n","    similarity_scores = torch.matmul(image_feats, torch.transpose(encoded_text, 0, 1))\n","    similarity_scores = similarity_scores.to(device)\n","    return similarity_scores.narrow(dim=-1, start=class_index, length=1).squeeze(dim=-1)\n","\n","def get_clip_probs(image_inputs, encoded_text, model, class_index=0):\n","    image_inputs = clip_input_transform(image_inputs).to(device)\n","    image_feats = model.encode_image(image_inputs).float()\n","    image_feats = F.normalize(image_feats, p=2, dim=-1)\n","\n","    clip_probs = (100.0 * torch.matmul(image_feats, torch.transpose(encoded_text, 0, 1))).softmax(dim=-1)\n","    clip_probs = clip_probs.to(device)\n","\n","    return clip_probs.narrow(dim=-1, start=class_index, length=1).squeeze(dim=-1)\n","\n","# Set up GAN\n","# Initialize GAN generator and transforms\n","with open(gan_model_path, 'rb') as f:\n","    G = pickle.load(f)['G_ema']\n","G.eval()\n","G.to(device)\n","latent_space_dim = G.z_dim\n","\n","# Set up clip classifier\n","clip_model_path = '/root/.cache/clip/ViT-B-32.pt'\n","clip_model = torch.jit.load(clip_model_path)\n","clip_model.eval()\n","clip_model.to(device)\n","input_resolution = clip_model.input_resolution.item()\n","context_length = clip_model.context_length.item()\n","vocab_size = clip_model.vocab_size.item()\n","\n","print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\")\n","print(\"Input resolution:\", input_resolution)\n","print(\"Context length:\", context_length)\n","print(\"Vocab size:\", vocab_size)\n","\n","# Extract text features for clip\n","\n","# Here is how we specify the desired attributes\n","# attributes = [\"an evil face\", \"an innocent face\"]\n","# class_index = 0 # which attribute do we want to maximize\n","\n","tokenizer = SimpleTokenizer(\"CLIP/clip/bpe_simple_vocab_16e6.txt.gz\")\n","sot_token = tokenizer.encoder['<|startoftext|>']\n","eot_token = tokenizer.encoder['<|endoftext|>']\n","text_descriptions = [f\"This is a photo of {label}\" for label in attributes]\n","text_tokens = [[sot_token] + tokenizer.encode(desc) + [eot_token] for desc in text_descriptions]\n","text_inputs = torch.zeros(len(text_tokens), clip_model.context_length, dtype=torch.long)\n","\n","for i, tokens in enumerate(text_tokens):\n","    text_inputs[i, :len(tokens)] = torch.tensor(tokens)\n","\n","# These are held constant through the optimization, akin to labels\n","text_inputs = text_inputs.to(device)\n","with torch.no_grad():\n","    text_features = clip_model.encode_text(text_inputs).float()\n","    text_features = F.normalize(text_features, p=2, dim=-1)\n","text_features.to(device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGv4MoraqzW5","executionInfo":{"status":"ok","timestamp":1679675329814,"user_tz":240,"elapsed":2348,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"be3fea08-a1a1-4b6f-a02c-ff741f2e4c7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model parameters: 151,277,313\n","Input resolution: 224\n","Context length: 77\n","Vocab size: 49408\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0115,  0.0090, -0.0371,  ..., -0.0420, -0.0333, -0.0091],\n","        [ 0.0325,  0.0153, -0.0423,  ..., -0.0113, -0.0088, -0.0020]],\n","       device='cuda:0')"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["# Setting up Transformer, i.e. the function that transforms the input z vector \n","# --------------------------------------------------------------------------------------------------------------\n","transformer_params = ['OneDirection', 'None']\n","transformer = transformer_params[0]\n","transformer_arguments = transformer_params[1]\n","if transformer_arguments != \"None\":\n","    key_value_pairs = transformer_arguments.split(\",\")\n","    key_value_pairs = [pair.split(\"=\") for pair in key_value_pairs]\n","    transformer_arguments = {pair[0]: pair[1] for pair in key_value_pairs}\n","else:\n","    transformer_arguments = {}\n","\n","transformation = getattr(transformations, transformer)(latent_space_dim, vocab_size, **transformer_arguments)\n","transformation = transformation.to(device)\n","\n","# function that is used to score the (attribute, image) pair\n","scoring_fun = get_clip_probs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66HoImoYq2RG","executionInfo":{"status":"ok","timestamp":1679675335969,"user_tz":240,"elapsed":1,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"0ac1a4b6-15ce-4ade-8158-c8ebebe5aeef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","approach:  one_direction\n","\n"]}]},{"cell_type":"code","source":["# Training\n","# --------------------------------------------------------------------------------------------------------------\n","# optimizer\n","optimizer = torch.optim.Adam(transformation.parameters(), lr=0.0002) #as specified in GANalyze\n","losses = common.AverageMeter(name='Loss')\n","\n","#  training settings\n","optim_iter = 0\n","batch_size = 6 # Do not change\n","train_alpha_a = -0.5 # Lower limit for step sizes\n","train_alpha_b = 0.5 # Upper limit for step sizes\n","num_samples = 10000 # Number of samples to train for # Ganalyze uses 400,000 samples. Use smaller number for testing.\n","\n","# create training set\n","#np.random.seed(seed=0)\n","truncation = 1\n","zs = common.truncated_z_sample(num_samples, latent_space_dim, truncation)\n","\n","# checkpoint_dir = f'checkpoints/results_maximize_{attributes[class_index]}_probability'\n","pathlib.Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n","\n","# loop over data batches\n","for batch_start in range(0, num_samples, batch_size):\n","\n","    # input batch\n","    s = slice(batch_start, min(num_samples, batch_start + batch_size))\n","    z = torch.from_numpy(zs[s]).type(torch.FloatTensor).to(device)\n","    y = None\n","    \n","    #step_sizes = (train_alpha_b - train_alpha_a)*np.ones(batch_size)*0.0001\n","    #print(step_sizes)\n","\n","    step_sizes = (train_alpha_b - train_alpha_a) * \\\n","        np.random.random(size=(batch_size)) + train_alpha_a  # sample step_sizes\n","    \n","    step_sizes_broadcast = np.repeat(step_sizes, latent_space_dim).reshape([batch_size, latent_space_dim])\n","    step_sizes_broadcast = torch.from_numpy(step_sizes_broadcast).type(torch.FloatTensor).to(device)\n","\n","    # ganalyze steps\n","    gan_images = G(z, None)\n","    gan_images = gan_output_transform(gan_images)\n","    out_scores = scoring_fun(\n","        image_inputs=gan_images, encoded_text=text_features, model=clip_model, class_index=class_index,\n","    )\n","    # TODO: ignore z vectors with less confident clip scores\n","    target_scores = out_scores + torch.from_numpy(step_sizes).to(device).float()\n","\n","    z_transformed = transformation.transform(z, None, step_sizes_broadcast)\n","    gan_images_transformed = G(z_transformed, None)\n","    gan_images_transformed = gan_output_transform(gan_images_transformed).to(device)\n","    out_scores_transformed = scoring_fun(\n","        image_inputs=gan_images_transformed, encoded_text=text_features, model=clip_model, class_index=class_index,\n","    ).to(device).float()\n","\n","    # compute loss\n","    loss = transformation.criterion(out_scores_transformed, target_scores)\n","\n","    # backwards\n","    loss.backward()\n","    optimizer.step()\n","\n","    # print loss\n","    losses.update(loss.item(), batch_size)\n","    if optim_iter % 10 == 0:\n","        logger.info(f'[Maximizing score for {attributes[class_index]}] Progress: [{batch_start}/{num_samples}] {losses}')\n","        print(f'[Maximizing score for {attributes[class_index]}] Progress: [{batch_start}/{num_samples}] {losses}')\n","        \n","\n","    if optim_iter % 50 == 0:\n","        logger.info(f\"saving checkpoint at iteration {optim_iter}\")\n","        print(f\"saving checkpoint at iteration {optim_iter}\")\n","        torch.save(transformation.state_dict(), os.path.join(checkpoint_dir, \"pytorch_model_{}.pth\".format(batch_start)))\n","\n","        # plot and save sample images\n","        plot_save_image(gan_images, checkpoint_dir)\n","\n","    optim_iter = optim_iter + 1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1pBrvnPTSTFVXHN3p00NyCMZNI8wHguPr"},"id":"pyUD6MXSrwJF","executionInfo":{"status":"error","timestamp":1679676317952,"user_tz":240,"elapsed":979947,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"f78c4a53-4ff5-4aeb-b366-3e13ecc44599"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Now that the model is trained, we can test it"],"metadata":{"id":"QkYR11tbMw6r"}},{"cell_type":"code","source":["# Testing\n","import torch_utils\n","import ganalyze_transformations as transformations\n","import ganalyze_common_utils as common\n","from clip_classifier_utils import SimpleTokenizer\n","# --------------------------------------------------------------------------------------------------------------\n","#TODO Figure out where to resume checkpoints\n","\n","def one_hot(index, vocab_size=1000):\n","    output = torch.zeros(index.size(0), vocab_size).to(index.device)\n","    output.scatter_(1, index.unsqueeze(-1), 1)\n","    return output\n","\n","\n","def denorm(x):\n","    out = (x + 1) / 2\n","    return out.clamp_(0, 1) * 255\n","\n","# helper function for visualization of test images\n","#def make_image(z, y, step_size, transform):\n","def make_image(z, step_size, transform):\n","\n","    if transform:\n","        z_transformed = transformation.transform(z,None, step_size)\n","        z_transformed = z.norm() * z_transformed / z_transformed.norm()\n","        z = z_transformed\n","\n","    #gan_images = utils.pytorch.denorm(generator(z, y))\n","    #gan_images = common.pytorch.denorm(G(z, y))\n","    #gan_images = G(z, None)\n","    gan_images = denorm(G(z, y))\n","    gan_images_np = gan_images.permute(0, 2, 3, 1).detach().cpu().numpy()\n","    gan_images = gan_output_transform(gan_images)\n","\n","    #gan_images = gan_images.view(-1, *gan_images.shape[-3:])\n","    gan_images = gan_images.to(device)\n","\n","    #out_scores_current = output_transform(assessor(gan_images))\n","    #out_scores_current = gan_output_transform(scoring_fun(gan_images))\n","    #TODO Check assessor section in Ganalyze test script line 106\n","    out_scores_current = scoring_fun(image_inputs=gan_images, encoded_text=text_features, model=clip_model, class_index=class_index).to(device).float()\n","    out_scores_current = out_scores_current.detach().cpu().numpy()\n","    if len(out_scores_current.shape) == 1:\n","        out_scores_current = np.expand_dims(out_scores_current, 1)\n","\n","    return(gan_images_np, z, out_scores_current)"],"metadata":{"id":"QF9gspXC622O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test settings\n","\n","#code from ganalyze_commons_utils added below \n","\n","import numpy as np\n","from scipy.stats import truncnorm\n","import PIL.ImageDraw\n","import PIL.ImageFont\n","\n","\n","def truncated_z_sample(batch_size, dim_z, truncation=1):\n","    values = truncnorm.rvs(-2, 2, size=(batch_size, dim_z))\n","    return truncation * values\n","\n","\n","def imgrid(imarray, cols=5, pad=1):\n","    if imarray.dtype != np.uint8:\n","        imarray = np.uint8(imarray)\n","        # raise ValueError('imgrid input imarray must be uint8')\n","    pad = int(pad)\n","    assert pad >= 0\n","    cols = int(cols)\n","    assert cols >= 1\n","    N, H, W, C = imarray.shape\n","    rows = int(np.ceil(N / float(cols)))\n","    batch_pad = rows * cols - N\n","    assert batch_pad >= 0\n","    post_pad = [batch_pad, pad, pad, 0]\n","    pad_arg = [[0, p] for p in post_pad]\n","    imarray = np.pad(imarray, pad_arg, 'constant', constant_values=255)\n","    H += pad\n","    W += pad\n","    grid = (imarray\n","            .reshape(rows, cols, H, W, C)\n","            .transpose(0, 2, 1, 3, 4)\n","            .reshape(rows * H, cols * W, C))\n","    if pad:\n","        grid = grid[:-pad, :-pad]\n","    return grid\n","\n","\n","def annotate_outscore(array, outscore):\n","    for i in range(array.shape[0]):\n","        I = PIL.Image.fromarray(np.uint8(array[i, :, :, :]))\n","        draw = PIL.ImageDraw.Draw(I)\n","        #font = PIL.ImageFont.truetype(\"arial.ttf\", int(array.shape[1]/8.5))\n","        font = PIL.ImageFont.load_default()\n","        message = str(round(np.squeeze(outscore)[i], 2))\n","        x, y = (0, 0)\n","        w, h = font.getsize(message)\n","        #print(w, h)\n","        draw.rectangle((x, y, x + w, y + h), fill='white')\n","        draw.text((x, y), message, fill=\"black\", font=font)\n","        array[i, :, :, :] = np.array(I)\n","    return(array)\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, name, fmt=':f'):\n","        self.name = name\n","        self.fmt = fmt\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","    def __str__(self):\n","        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n","        return fmtstr.format(**self.__dict__)\n","\n","#Test settings\n","\n","num_samples = 10\n","truncation = 1\n","iters = 3\n","#np.random.seed(seed=999) #removed like training code block\n","annotate = True\n","# vocab_size = 1 #vocab size is one for debugging. else change to:\n","vocab_size = clip_model.vocab_size.item()\n","\n","if vocab_size == 0:\n","    num_categories = 1\n","else:\n","    num_categories = vocab_size\n","\n","for y in range(num_categories):\n","\n","    ims = []\n","    outscores = []\n","\n","    #zs = utils.common.truncated_z_sample(num_samples, dim_z, truncation)        \n","    #zs = common.truncated_z_sample(num_samples, latent_space_dim, truncation)\n","    zs = truncated_z_sample(num_samples, latent_space_dim, truncation)\n","\n","    ys = np.repeat(y, num_samples)\n","    zs = torch.from_numpy(zs).type(torch.FloatTensor).to(device)\n","    ys = torch.from_numpy(ys).to(device)\n","    ys = one_hot(ys, vocab_size)\n","    #step_sizes = np.repeat(np.array(opts[\"alpha\"]), num_samples * dim_z).reshape([num_samples, dim_z])\n","\n","    alpha = 0.2\n","    step_sizes = np.repeat((alpha), num_samples * latent_space_dim).reshape([num_samples, latent_space_dim])\n","\n","    # TODO instead write a loop here to sample values of alpha\n","    #alpha= [-0.5,-0.25,0,0.25,0.5]\n","    #step_sizes = np.repeat(np.array[of alpha], num_samples * latent_space_dim).reshape([num_samples, latent_space_dim])\n","\n","    step_sizes = torch.from_numpy(step_sizes).type(torch.FloatTensor).to(device)\n","    feed_dicts = []\n","    for batch_start in range(0, num_samples, 4):\n","        s = slice(batch_start, min(num_samples, batch_start + 4))\n","        #feed_dicts.append({\"z\": zs[s], \"y\": ys[s], \"truncation\": truncation, \"step_sizes\": step_sizes[s]})\n","        feed_dicts.append({\"z\": zs[s], \"truncation\": truncation, \"step_sizes\": step_sizes[s]})\n","\n","\n","    for feed_dict in feed_dicts:\n","        ims_batch = []\n","        outscores_batch = []\n","        z_start = feed_dict[\"z\"]\n","\n","        step_sizes = feed_dict[\"step_sizes\"]\n","        \n","        #if opts[\"mode\"] == \"iterative\": \n","        # choose from iterative or bigger_step\n","        mode = \"iterative\"\n","        if mode == \"iterative\":\n","            print(\"iterative\")\n","\n","            # original seed image\n","            #x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n","            x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"step_sizes\"], transform=False)\n","\n","            x = np.uint8(x)\n","            if annotate:\n","                #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n","                #ims_batch.append(common.annotate_outscore(x, outscore))\n","                ims_batch.append(annotate_outscore(x, outscore))\n","\n","\n","            else:\n","                if annotate:\n","                    #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n","                    #ims_batch.append(common.annotate_outscore(x, outscore))\n","                    ims_batch.append(annotate_outscore(x, outscore))\n","\n","\n","                else:\n","                    ims_batch.append(x)\n","            outscores_batch.append(outscore)\n","\n","            # negative clone images\n","            z_next = z_start\n","            step_sizes = -step_sizes\n","            for iter in range(0, iters, 1):\n","                feed_dict[\"step_sizes\"] = step_sizes\n","                feed_dict[\"z\"] = z_next\n","                #x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n","                x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"step_sizes\"], transform=True)\n","                x = np.uint8(x)\n","                z_next = tmp\n","                if annotate:\n","                    #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n","                    #ims_batch.append(common.annotate_outscore(x, outscore))\n","                    ims_batch.append(annotate_outscore(x, outscore))\n","\n","                else:\n","                    if annotate:\n","                        #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n","                        ims_batch.append(common.annotate_outscore(x, outscore))\n","\n","                    else:\n","                        ims_batch.append(x)\n","                outscores_batch.append(outscore)\n","\n","            ims_batch.reverse()\n","\n","            # positive clone images\n","            step_sizes = -step_sizes\n","            z_next = z_start\n","            for iter in range(0, iters, 1):\n","                feed_dict[\"step_sizes\"] = step_sizes\n","                feed_dict[\"z\"] = z_next\n","\n","                #x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n","                x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"step_sizes\"], transform=True)\n","                \n","                x = np.uint8(x)\n","                z_next = tmp\n","\n","                if annotate:\n","                    #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n","                    ims_batch.append(annotate_outscore(x, outscore))\n","\n","                else:\n","                    ims_batch.append(x)\n","                outscores_batch.append(outscore)\n","\n","        else:\n","            print(\"bigger_step\")\n","\n","            # original seed image\n","            #x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=False)\n","            x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"step_sizes\"], transform=False)\n","            x = np.uint8(x)\n","            if annotate:\n","                ims_batch.append(utils.common.annotate_outscore(x, outscore))\n","            else:\n","                ims_batch.append(x)\n","            outscores_batch.append(outscore)\n","\n","            # negative clone images\n","            step_sizes = -step_sizes\n","            for iter in range(0, iters, 1):\n","                feed_dict[\"step_sizes\"] = step_sizes * (iter + 1)\n","\n","                #x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n","                x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"step_sizes\"], transform=True)\n","\n","                x = np.uint8(x)\n","\n","                if annotate:\n","                    #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n","                    ims_batch.append(annotate_outscore(x, outscore))\n","                else:\n","                    ims_batch.append(x)\n","                outscores_batch.append(outscore)\n","\n","            ims_batch.reverse()\n","            outscores_batch.reverse()\n","\n","            # positive clone images\n","            step_sizes = -step_sizes\n","            for iter in range(0, iters, 1):\n","                feed_dict[\"step_sizes\"] = step_sizes * (iter + 1)\n","\n","                #x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n","                x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"step_sizes\"], transform=True)\n","                x = np.uint8(x)\n","                if annotate:\n","                    #ims_batch.append(utils.common.annotate_outscore(x, outscore))\n","                    ims_batch.append(annotate_outscore(x, outscore))\n","\n","                else:\n","                    ims_batch.append(x)\n","                outscores_batch.append(outscore)\n","\n","        ims_batch = [np.expand_dims(im, 0) for im in ims_batch]\n","        ims_batch = np.concatenate(ims_batch, axis=0)\n","        ims_batch = np.transpose(ims_batch, (1, 0, 2, 3, 4))\n","        ims.append(ims_batch)\n","\n","        outscores_batch = [np.expand_dims(outscore, 0) for outscore in outscores_batch]\n","        outscores_batch = np.concatenate(outscores_batch, axis=0)\n","        outscores_batch = np.transpose(outscores_batch, (1, 0, 2))\n","        outscores.append(outscores_batch)\n","\n","    ims = np.concatenate(ims, axis=0)\n","    outscores = np.concatenate(outscores, axis=0)\n","    ims_final = np.reshape(ims, (ims.shape[0] * ims.shape[1], ims.shape[2], ims.shape[3], ims.shape[4]))\n","    #I = PIL.Image.fromarray(utils.common.imgrid(ims_final, cols=iters * 2 + 1))\n","    I = PIL.Image.fromarray(common.imgrid(ims_final, cols=iters * 2 + 1))\n","    \n","    #TODO change the code below to write a new file for every result cycle. Currently it over writes the my_results.jpg file. \n","    # For now, make sure to download your result \"my_results.jpg\" before running again\n","    # result_dir = \"my_results\" + str(y)\n","    # I.save(os.path.join(result_dir + \".jpg\"))\n","\n","    #I.save(os.path.join(result_dir, categories[y] + \".jpg\"))\n","    result_dir = \"/content/drive/MyDrive/evilcat/steering_images/cute_cat\"\n","    if not os.path.exists(result_dir):\n","        os.makedirs(result_dir)\n","\n","    # assume \"I\" is the image you want to save\n","    image_name = \"my_results\"+ \"_20000_\" + str(y) +\".jpg\"\n","    image_path = os.path.join(result_dir, image_name)\n","\n","    # save the image to the specified path\n","    I.save(image_path)\n","    print(\"y: \", y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bBALFuVwC8PH","executionInfo":{"status":"error","timestamp":1679676419357,"user_tz":240,"elapsed":52433,"user":{"displayName":"谢北辰","userId":"06281340701736441026"}},"outputId":"97679a68-2891-452c-ea8a-f6c83542003d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iterative\n","iterative\n","iterative\n","y:  0\n","iterative\n","iterative\n","iterative\n","y:  1\n","iterative\n","iterative\n","iterative\n","y:  2\n","iterative\n","iterative\n","iterative\n","y:  3\n","iterative\n","iterative\n","iterative\n","y:  4\n","iterative\n","iterative\n","iterative\n","y:  5\n","iterative\n","iterative\n","iterative\n","y:  6\n","iterative\n","iterative\n","iterative\n","y:  7\n","iterative\n","iterative\n","iterative\n","y:  8\n","iterative\n","iterative\n","iterative\n","y:  9\n","iterative\n","iterative\n","iterative\n","y:  10\n","iterative\n","iterative\n","iterative\n","y:  11\n","iterative\n","iterative\n","iterative\n","y:  12\n","iterative\n","iterative\n","iterative\n","y:  13\n","iterative\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-9517940760ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mannotate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0;31m#ims_batch.append(utils.common.annotate_outscore(x, outscore))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                     \u001b[0mims_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotate_outscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-47-9517940760ff>\u001b[0m in \u001b[0;36mannotate_outscore\u001b[0;34m(array, outscore)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mannotate_outscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mdraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageDraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#font = PIL.ImageFont.truetype(\"arial.ttf\", int(array.shape[1]/8.5))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tobytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}